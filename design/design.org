#+TITLE: Push Job Specification
#+AUTHOR: Christopher Brown <cb@opscode.com>\\Kevin Smith <kevin@opscode.com>\\Mark Anderson <mark@opscode.com>
#+OPTIONS: toc:nil
#+EXPORT_DATE: nil
#+OPTIONS: ^:{}
#+LaTeX: \raggedright
#+LaTeX: \thispagestyle{fancy}
#+LaTeX_HEADER: \hypersetup{colorlinks=true,linkcolor=blue,linkbordercolor=blue}
#+LaTeX_HEADER: \def\@pdfborder{0 0 1}
#+LaTeX_HEADER: \def\@pdfborderstyle{/S/U/W 1}}
#+LaTeX_HEADER: \usepackage[T1]{fontenc}
#+LaTeX_HEADER: \usepackage[adobe-utopia]{mathdesign}
#+LaTeX_HEADER: \usepackage[scaled]{berasans}
#+LaTeX_HEADER: \usepackage[scaled]{beramono}
#+LATEX_HEADER: \setlength{\evensidemargin}{0in}
#+LATEX_HEADER: \setlength{\oddsidemargin}{0in}
#+LATEX_HEADER: \setlength{\textwidth}{6.5in}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage{paralist}
#+LATEX_HEADER: \pagestyle{fancy} \renewcommand{\headrulewidth}{0pt}
#+LATEX_HEADER: \chead{\includegraphics[width=4cm]{Opscode_Logo_Small.png}}
#+LATEX_HEADER: \lhead{} \rhead{} \lfoot{\today}
#+LATEX_HEADER: \cfoot{Opscode Confidential}\rfoot{\thepage}
#+LATEX_HEADER: \renewcommand{\headrulewidth}{0pt}

* Overview
Opscode Chef is currently missing a key feature that comes up in discussion with nearly
every potential customer.  That feature is immediate, ad-hoc task execution against one or
more managed devices (nodes).  In Chef, we will model these tasks as /run lists/ and will
model their execution by distributed, coordinated /chef-client/ runs. For brevity's sake
this feature will be referred to as /push jobs/.

The concept of /push jobs/ is quite simple. A user is able to select some subset of nodes
managed by their Private Chef organization, specify an action or command for those nodes
to execute, and track the status of each node as it executes the request.

A bit of complexity and bookkeeping lurks underneath push job's simplicity. Managed nodes
will need to reliably monitor the job coordinators to cope with outages.  Job coordinators
also need to monitor the status of each managed node so commands are only sent to available,
responding nodes and to track job progress.

The push job server and the managed nodes ensure mutual liveness via a bidirectional
heartbeating system. The push job server sends heartbeat messages to all the managed nodes,
and the managed nodes send heartbeat messages to the server.

The remainder of this document will attempt to describe this feature in enough detail to allow
a reasonable and scalable implementation.

* Assumptions
** Connectivity
   1. Managed nodes *MUST* be reachable via a TCP-enabled network interface.
   2. Managed nodes *MUST* be able to *accept* incoming TCP
      connections. NOTE: This precludes implementation in current
      Hosted Chef. See below.
   3. Managed nodes *MUST* be able to connect to the heartbeat and job coordination
      components inside Chef server.
** Data format & Storage
   1. All messages will be formatted as legal JSON.
   2. The database is the canonical store of all application data.
** Scalability & Security
   1. Push jobs will be deployed in Private Chef only.
   2. Push jobs will not be deployed in Hosted Chef.
   3. The design must scale up to 8,000 managed nodes per OPC server.
   4. Push jobs will honor the same security guarantees made by the Chef REST API.
* Architecture
** Communications
Managed nodes and server components will communicate using [[http://www.zeromq.org][ZeroMQ]] messaging. While the server
will be bound to predefined ports, the node should never require them. This is to ease running
multiple nodes on the same host for scalability testing; we will want to be able to run many
hundreds of nodes on the same machine rather than stand up thousands of individual nodes to
test message scalability.

Communication can be separated into two categories: heartbeat and job execution. The heartbeat
channel is used by the Chef server to detect when managed nodes are offline and, therefore,
unavailable for job execution. Managed nodes also use the heartbeat channel to detect when the
server is unavailable.

The job execution channel is used by the Chef server to send job execution requests to
managed nodes. Managed nodes used the execution channel to send job-related messages such
as acknowledging jobs, sending progress updates, and reporting final results.

*** Heartbeat Channel
ZeroMQ PUB/SUB (publish and subscribe) sockets are used because they automatically and
scalably manage the fan-in the server requires to monitor nodes as well as the fan-out the
server needs to broadcast its heartbeat to all the nodes.

In the interest of simplicity, the heartbeat protocol should be as stateless as possible. The
server continually broadcasts heartbeat messages to all nodes that are listening. Nodes
indicate their availability by sending heartbeat messages to the server. There is no process
beyond the ZeroMQ connection to start or stop a connection from the node.

This might be worth modifying in the future. For example we might want to lower the signature
validation load on the server by having a per-session symmetric key for the node heartbeat
established at startup. Instead of the somewhat expensive public key signature check we could
simply decrypt the packet with the session key and check for sanity. (TODO: think about whether
this protocol is actually sane and secure)

#+CAPTION: ZeroMQ sockets
#+LABEL: img:heartbeat.jpg
[[./heartbeat.jpg]]

The details of how nodes and servers discover and connect to each other's PUB and SUB sockets
is covered in [[Server and Node Discovery]].
#+LaTeX: \pagebreak

*** Command Channel
    TBD

** Configuration/Discovery Process
   The configuration and service discovery process will provide the following pieces of data:
   * The push job server hostname or address
   * The port to subscribe to for server heartbeat
   * The port to push node heartbeats to
   * The public key of the server
   * The lifetime of this configuration information

   A configuration endpoint ':organization/nodes/push_jobs' will be added to our Chef REST
   services. A signed /GET/ to this endpoint will retrieve the appropriate configuration information
   in JSON format.

   #+begin_src javascript
    {
      "type": "config",
      "host": "opc1.opscode.com",
      "push_jobs": {
                     "heartbeat": {
                                    "out_addr": "tcp://10.10.1.5:10000",
                                    "in_addr": "tcp://10.10.1.5:10001",
                                    "interval": 15,
                                    "offline_threshold": 3,
                                    "online_threshold": 2
                                  },
                     "command" : {
                                    "command_source":"tcp://10.10.1.5:10002"
                                 }
                   },
      "public_key": "AAAAB3NzaC1kc3MAAACBAIZbwlySffbB
                    5msSUH8JzLLXo/v03JBCWr13fVTjWYpc
                    cdbi/xL3IK/Jw8Rm3bGhnpwCAqBtsLvZ
                    OcqXrc2XuKBYjiKWzigBMC7wC9dUDGwDl
                    2aZ89B0jn2QPRWZuCAkxm6sKpefu++VPR
                    RZF+iyZqFwS0wVKtl97T0gwWlzAJYpAAA
                    AFQDIipDNo83e8RRp7Fits0DSy0DCpwAA
                    AIB01BwXg9WSfU0mwzz/0+5Gb/TMAxfkD
                    yucbcpJNncpRtr9Jb+9GjeZIbqkBQAqwg
                    dbEjviRbUAuSawNSCdtnMgWD2NXkBKEde",
       "lifetime":3600

    }
    #+end_src

    + type :: message type
    + host :: sender's host name (Private Chef server)
    + push\_jobs/heartbeat/out_addr :: URL pointing to the server's heartbeat broadcast service
    + push\_jobs/heartbeat/in_addr :: URL pointing to the server's node state tracking service
    + push\_jobs/interval :: Interval, in seconds, between heartbeat messages
    + push\_jobs/offline_threshold :: How many intervals must be missed before the other end is considered offline
    + push\_jobs/online_threshold :: How many intervals must be missed before the other end is considered online
    + public_key :: The signing key that the push server will use.
    + lifetime :: how long in seconds this configuration is good for. The node should reload the
                  configuration information after this has expired.

   We may wish to use the discovery process to handle fail over to a new server and distribution of
   nodes among multiple servers. The discovery system would allocate the nodes to various active servers
   and if a node lost the server heartbeat for a certain length of time (or got a reconfigure
   command via the command channel) it would reload the configuration and start a connection to the
   appropriate server. We would also reconfigure after the lifetime of the configuration expires.


** General Messaging
*** JSON
   Push jobs use JSON because ZeroMQ sends and receives messages as complete frames, without
   fragmentation. JSON also facilitates easier debugging and maintenance of the system since
   all messages are textual and human-readable. A binary protocol, such as /Protocol Buffers/
   or /msgpack/, would be more efficient but would also substantially increase the effort
   required to debug and support the system.  We can discuss those as potential
   optimizations once the initial system is in place.
*** Security
   All messages are signed using the caller's private key. This signature is transmitted in
   a separate ZeroMQ frame before the JSON payload.[fn:1]
   #+begin_src erlang
   Sock = connect_to_server("tcp://some_server:8765"),
   Sig = sign_message(JSON),
   erlzmq:send(Sock, Sig, [sndmore]),
   erlzmq:send(Sock, JSON)
   #+end_src
** Heartbeat
    Liveness detection in a distributed system is a notoriously difficult problem. The most common
    approach is to arrange for two parties to exchange heartbeat messages on a regular interval. Let's
    call these two parties 'A' and 'B'. Both A and B are considered 'online' while they are able to
    exchange heartbeat messages. If A fails to receive heartbeats from B for some number of consecutive
    intervals then A will consider B 'offline' and not route any traffic to B. A will update B's
    status to 'online' once A starts receiving heartbeats from B again.

    The protocol described here is loosely based on the Paranoid Pirate Protocol, but with some
    embellishments introduced for signing.  

    The heartbeat server sends out regular heartbeats to managed nodes via ZeroMQ
    PUB/SUB. Managed nodes send their heartbeats over a separate channel. See the [[Heartbeat
    Channel]] section for a visual representation of the message flows and ZeroMQ sockets.

*** Message Format
    The basic message format used here is a simple header frame containing the protocol
    version and a signature separated by CRLF.

    The main frame is a JSON blob. Push jobs use JSON because ZeroMQ sends and receives
    messages as complete frames without fragmentation. JSON also facilitates easier
    debugging and maintenance of the system since all messages are textual and
    human-readable. A binary protocol, such as /Protocol Buffers/ or /msgpack/, would be
    more efficient but would also substantially increase the effort required to debug and
    support the system.

*** Security
    All messages are signed using the caller's private key. This signature is transmitted in
    a separate ZeroMQ frame before the JSON payload. The actual payload is not encrypted,
    and is broadcast to all nodes. The system should never broadcast any data that is
    sensitive, such as commands or node status. This implies that the server heartbeat
    broadcast is not suitable for commands.

*** Socket configuration
    The heartbeats (and other messages) flowing through the system are time sensitive. There is
    little value keeping many more packets than the online/offline threshold values. If we go too
    long without receiving a heartbeat, we will be declaring the machine down anyways. Furthermore,
    the signing protocol will mandate the rejection of aged packets.

    This implies that the HWM values should be kept small, and ZMQ_SWAP should always be zero.

*** Server Heartbeat Channel
    PUB/SUB sockets are used for the server heartbeat because this manages the fan-out
    required to send messages to thousands of nodes. The node subscribes to the server
    heartbeat at a host/port combination specified in the discovery process.

    The HWM should be kept small; there is no point in storing messages for dead nodes any
    longer than necessary. Node failure must be accepted and tolerated. If a node has
    been marked as down (not reachable), we want to drop any messages destined for that
    node. This is in keeping with the fail-fast philosophy.

    The nodes do not ACK the server heartbeats, and the server should not expect any.
*** Server Heartbeat Message
    First frame (Why not break each of these into separate frames and take
    advantage of 0mq's multi-part for parsing the header as well?)
    #+begin_example
Version=1.0;Checksum=fyq6ukIwYcUJ9JI90Ets8Q==
# Version -> a decimal ASCII integer value for the protocol version (1 for now)
# SignedChecksum -> the signed checksum of the second packet in hexadecimal (base64 could be another option as it 2/3 the size)
    #+end_example
    Second packet is json
    #+begin_example
{"server":"SERVER",                 # specifies the server
 "sequence":SEQUENCE_NUMBER",       # integer sequence number
 "timestamp":"TIMESTAMP",           # timestamp
 "type":"MSGTYPE"                   # 'heartbeat' for now
}
    #+end_example


*** Node Heartbeat Channel
    PUSH/PULL sockets are used for the node heartbeat. The node PUSHes heartbeats to the
    server at the host/port specified in the config data received during [[Server and Client Discovery][discovery]]. The
    server will not ACK heartbeats.

    NOTE: Some versions of this spec had PUB/SUB being used for this process. It would be
    simpler if the node was able to connect to the server to send heartbeats, rather than
    requiring the server to bind to the node. The latter would require some sort of
    handshake on startup to inform the server where to connect. While it is possible to bind
    the SUB to an address and connect the PUB, this seems to be not recommended (see ZeroMQ
    guide, 'Getting the Message Out'). However, it seems that multiple PUSH to one PULL is
    supported, and we can bind the PULL socket to an address without trouble.

    There isn't any reason we couldn't use the heartbeat to convey extra information. The
    public key signature-based authentication process for heartbeats already requires a
    moderate sized payload, so a little extra information seems pretty harmless. This is in
    contrast to the 1-2 byte sized payload in the paranoid pirate protocol. Possible items to
    include are:

   * The port the command processor is listening on.
   * ID and status of the most recently received command.
   * Information allowing the detection of crashed nodes
*** Node Heartbeat Message
    Node heartbeats are comprised of two ZeroMQ frames. The first frame contains
    the signature version and signature for the heartbeat message:

#+begin_example
    VersionId=1.0;Checksum=CUtCvLKeS0l/fq5lnGf25w==
#+end_example

    The second frame contains the JSON-formatted heartbeat payload:

#+begin_example
    {"node": "node123.foo.com",                    # node's host name
     "org": "foo.com",                             # orgname of the node
     "timestamp": "Sat, 12 May 2012 20:33:15 GMT", # timestamp
     "state": "idle"
    }
#+end_example

   The node will discontinue the heartbeat and note the server as down if the server heartbeat
   state moves to down, and resume it when the server heartbeat resumes. 

   A managed node must mark the OPC server as offline when it fails to receive server heartbeats for
   a consecutive number of intervals equal to push\_jobs/heartbeat/offline\_threshold. A managed
   node must not attempt to send any data when the server is offline. Any job requests received by
   the managed node from the offline server which haven't begun execution must be discarded.
 
   After a managed node has marked the server as offline it must receive server heartbeats for a consecutive
   number of intervals equal to push\_jobs/heartbeat/online\_threshold before marking the server online.
   The managed node may resume sending data and accepting job requests from the OPC server at this point.

   If the node fails to receive a heartbeat for too long, it will query the configuration
   interface to receive a possible configuration update. This would allow the system to recover from
   a failed server.

   The node may wish to detect if the HWM is reached on the PUSH socket, since it will block when the
   HWM is reached. One strategy would be to set the HWM low and have some sort of alarm detect if we
   are blocked for any length of time. If the HWM is reached, we should declare the server down as
   if it stopped sending heartbeats. 

   The node can report a variety of states. 
 
   idle: not running any jobs
   ready: reports job id in flight
   running: also reports job id
   restarting: about to restart

**** TODO Identify other possible states to report.   


*** Server tracking of node 


*** Simple protocol for detection of crashed node
    It can be helpful to know whether a node has crashed and returned (possibly on different
    hardware) vs undergone a planned restart. This can be done with a guid and simple state file
    (/var/pushy/incarnation). On startup, the client will look for the incarnation file, load the
    incarnation GUID from it, and delete the file. If no file is found, the client will generate a
    new incarnation GUID. On a clean shutdown the current incarnation GUID is written to the
    file. The client reports this incarnation GUID in its heartbeat, and if the incarnation id
    changes the push job server can recognize this and act accordingly. If a command was in flight
    the server should record that it ended in a indeterminate state.

** Node-Server Command Execution
   A OPC server sends requests to execute actions to managed nodes. These requests are called
   commands. The command server listens on an address specified in the configuration process, and
   clients connect to that address to receive commands.

*** Vocabulary
    * Job - A collection of nodes executing the same command. Jobs have persistent state.
      This state can be queried by users via the knife 'job' command.
    * Command - A request to a managed node to perform an action immediately.

*** Overall communications structure

   The command server will create a ROUTER socket, and each client will connect via a DEALER socket
   identified with a name unique to that client. On connection to the ROUTER socket, the client will
   send a signed message indicating that it is available for commands, letting us capture the
   transient socket name. This will provide a way to map the unique name of the connection to the
   client in question. Commands will be addressed via that unique name. The client will learn the
   the address of the ROUTER socket via the discovery system.

   Alternately we could name the sockets with a name derived from the organization and the node
   name, and possibly skip the first message exchange. However this creates a persistent connection,
   with possible issues with messages queued to be send to dead nodes, and stale commands showing up
   when a new node with the same name appears. 
   
   The server will need to send a separate message to each client. However the message body
   (aside from the address packet) will remain the same, and we can reuse the ZeroMQ buffers created
   and save on the signing cost. 

   When a command is completed on the node, it sends the command results back to the server using
   the ROUTER/DEALER connection above. 


*** TODO What happens if we send a command to a node that isn't coming back; 
**** does it occupy space in the zmq buffers until the server restarts things? 
**** do we want to periodically drop and restart the ZMQ ROUTER socket to purge these?    
*** TODO Devise a mechanisim for job cancellation

*** Command vocabulary
**** chef-client
      This command causes a managed node to initiate a chef-client run.
**** ohai
      This command tells a managed node to run ohai and update its node data.
      This is not required for the PoC project but will be required for the real
      implementation.
**** sleep n
      This command tells a managed node to wait n seconds and then reply with success. This is
      intended for testing.
**** dummy_job PFAIL DURATION
      This would be a more sophisticated version of the sleep command. This command tells a managed
      node to wait duration seconds and either fail with a probability of pfail, and otherwise
      succeed. This would ease testing of the system failure cases.
**** arbitrary command?
      This is the most general possible solution, and something we should consider for the long
      term; apparently (according to Lamont) just about every company has some provision for this in
      their infrastructure. Many interesting security issues appear, including what UID to run under
      (root? the user id that knife ssh would use?) access control problems, etc. Perhaps this needs
      to have a specific ACL right in authz.
**** abort JOBID
      The final product will need some mechanism to cancel/abort a job in progess. The job id may
      not be necessary since there will only be one job running per client. (However it might be
      nice if the client and server get out of sync, and the client thinks its running a different
      job than the server does)

*** Persistence and lifetimes. 
   Jobs need to be persistent on the server side; we should not 'forget' a job in progress even if
   the server restarts. However, clients may lose state at any time, and we must be prepared to
   handle it. 
   
   Cases to consider:

**** A node crashes in the middle of job execution, and loses all knowledge of the current job state.
**** A long lost node reconnects with job status or execution process long after the job is over.
**** Can a node do a controlled reboot as part of command execution? 
     This comes in play for such items as kernel upgrades. If so, we will need some way to identify
     completion, presumably when the node boots back up. This will require the client to keep some
     state through a reboot.

*** TODO How do we handle stuck/dead nodes?
*** TODO What is the default duration for a job?
*** TODO Does OPC have a notion of max #/concurrently executing push commands?
    As SF has commented, we have the potential to make push jobs a DDOS tool for OPC. We probably
    will need to refine our system to track clients in-flight, and limit the peak # of clients in
    flight. This complicates the push job system a bit, in that we will have to have a mechanism to
    track in-flight items and issue commands. 

    This has implications for the job lifetime; we may want separate values and management of the
    lifetime of the job overall, and the lifetime of the command execution on an individual node. 

*** TODO How does the job monitor monitor the up/down state of each participating node?


*** Commands and node state
     A server may not send commands to a down node. A server may not send a command to a node which
     is currently executing a command. In other words, nodes execute serially only. This makes it
     easier to reason about the current state of any node and also avoids any undesired runtime
     interactions between commands.
    
*** TODO
*** TODO How do we handle job queueing for backed up nodes? No more than 1-2 per node? 
    MAA: I believe we should *not* do queueing for jobs. 
    MAA: Do jobs issue multiple commands? I think the natural primitive is for a job to issue a
    single command, with a single up/down result. Multi-command sequences are assembled at the job level.

*** Server-side job execution lifecycle
**** Initiation
***** Search & Filter
     Command execution is tracked across a collection of nodes by a command job. The collection of
     nodes is defined by a standard Chef search criteria. The search is executed against the node
     index and returns a list of managed nodes satisfying the search criteria. The search criteria
     should eventually allow up/down state and job execution state as factors. For example it would
     be quite valuable to run a job against all nodes that match a search and 'succeeded' on job id
     XXXX.

     MAA: Do we need search for prototyping? Can we replace it with an oracle that maps a text
     string to a node list? E.g. maybe the input to the 'search' is the list of nodes to use? This
     seperates

     MAA: CB mentions the idea of entity groups (a named collection of nodes or other entities in
     chef); in the future we may want to integrate this more deeply.

***** Command Transmission
     By default the job shall be executed on all candidates returned by the search
     and filter step. Exceptions to this and user-visible ways to modify the
     default behavior are discussed later in this document.

     When the OPC server is ready to execute a job it creates a job monitor. The role
     of the job monitor is to manage the execution of a job and track the execution
     state of all nodes referenced by the job.

****** Command message format
       The job command message is extremely similar to the heartbeat message. It
       consists of two ZeroMQ frames. The first frame contains the signature version and
       signature of the message.

#+begin_example
Version=1.0;Checksum=vQiGY85BzEAi5k7noSiN3A==
#+end_example

       The second frame contains the command payload for the job formatted as a JSON
       hash. The payload contains the job id, command to be executed, the host name
       of the OPC server, and the URL of the PULL socket used to collect node ACK/NAKs.

#+begin_example
{
  "server": "opc1.foo.com",               # OPC host name
  "type": "job_command",                  # message type
  "job_id": "1234",                       # job id
  "command": "chef-client"                # command to execute
}
#+end_example

       On receipt of this command the client will reply with an ACK message and will be marked as
       ready for execution. If the client is currently executing a job, it will reply with a NACK
       message and the server will mark that node as ineligible to start the command.

***** Modifying Job Initiation
      Users can place additional restrictions on the initiation of a push job.
      These restrictions are expressed by passing additional flags to the knife
      job command.
****** Quorum
       A user can specify that a minimum number of nodes matching the search criteria
       must both be online and ACK the job request. This is done by using the quorum
       flag.
#+begin_example
knife job create role:database chef-client --quorum=0.8
#+end_example

#+begin_example
knife job create role:database chef-client --quorum=3
#+end_example

       The first example illustrates specifying quorum as a percentage of the total
       number of nodes returned from the search index.

       The second example illustrates specifying quorum as an absolute count of nodes.

****** Lifetime
       A job will have a specific lifetime; if execution has not completed by the timeout, nodes
       with a command in flight will be aborted and the job state will be marked as timed out. There
       should also be a default lifetime of a job set to some TBD value. (Is an hour a reasonable
       time? Most chef-client runs should be done by then). There are obvious tradeoffs between
       squelching laggarts and not being too hasty.

****** Concurrency
       In many cases we will want to limit how many simultaneous nodes are executing a job. This
       will complicate the job manager, as it will need to track nodes completing or timing out and
       start new nodes.

**** Executing the job
     Once a job meets the job initiation criteria it is ready to execute. The job monitor
     tells the participating nodes to begin execution by sending the execute message to
     all ACK'd nodes.
***** Execute message format
      As with prior messages, the execute message is comprised of two ZeroMQ frames.
      The first frame contains the digital signature and signature version as previously
      illustrated. The second frame contains the execute message payload.

#+begin_example
{
  "server": "opc1.foo.com",    # OPC host name
  "job_id": "1234",            # job id
  "type": "job_execute"        # message type
}
#+end_example
**** Tracking job status
      If at least one node has ACK'd the job request (quorum requirements withstanding, see
      subsequent sections) the monitor then transitions the job to an executing state.  The monitor
      is responsible for updating the job details as each participating node sends its job execution
      results. Since OPC already has other mechanisms for collecting the output of chef-client runs,
      nodes are required to report only pass/fail status.

***** Job States

      * Executing - At least one node ACK'd the job request and is executing the command.
      * Complete - All participating nodes executed the command without errors.
      * Error - At least one node encountered errors while executing the command.
      * Failed - All candidate nodes NAK'd the job request OR no candidate nodes were found.
      * Expired - At least one node failed to complete in the specified time.
      * Aborted - The job was aborted
***** Per node job information

      There are several broad states a node can be in, and subcategories of these
      * The command never started.
	* it was down when the command initiated
        * it never ACKed
        * it NACKed, because it was busy with some other job (perhaps record which job)
      * The command started, but has not (yet) completed
        * the command is in flight, and everything seems ok (heartbeats are recieved)
	* the command is in flight, and something is wrong (no heartbeat, cancelling the command, etc)
      * The command stopped for some without completing successfully.
        * failed
        * timed out
        * was aborted
        * node died 
      * The command completed successfully

****** TODO: Figure out how to handle heartbeat loss during command execution; we don't know whether the command completed or not. 

MARKER--- revisions haven't gone past here yet.


      We will want to keep information about the state of each node in a job. 

      * Ready - available for a command, but the command not yet sent.
      * Execution - executing command
      * Down - not issued the command because it was down
      * Completed - sucessfully completed the command
      * Failed - failed during execution of the command
        Possible subtypes include
      ** Error - there was an error in the execution of the command
      ** Crashed - not only did the command fail, but the node went down during execution
      ** Timed out - the command took longer than its time limit, and was terminated
      * Aborted - the command was running when an abort message was recieved.

      This information will be useful when chaining together multiple jobs.

****** Job Expiry
       Users can also specify a maximum duration for a job. This is accomplished by passing
       the duration flag to the knife job plugin. Duration is always expressed in minutes.

#+begin_example
knife job create role:database chef-client --duration=10
#+end_example

***** 
*** Node-side job execution lifecycle
**** Initiation
     ACK if idle. NAK if job is currently running and the server asked node to start
     another one. If ACK, consider the job to have started execution for the purposed
     of future job requests.
**** Execution
     Wait for server to send execution message. Begin command execution when message is
     received.
**** Abort
     
**** Completion
     Send success or error message when command completes.

*** REST API for job control
    Job control will be done via a rest api in the pushy server. API requests are signed like any
    other chef command.
**** ORGNAME/pushy/jobs GET
     List all jobs 
**** ORGNAME/pushy/jobs PUT
     Start a job. The body will be a json blob specifying the list of nodes and the command to run, along with any other
     parameters (throttling, lifetime, etc). The return value is the guid of the job.
**** ORGNAME/pushy/jobs POST
     Amend a job; this may include aborts, or alterations of timeouts. 
**** ORGNAME/pushy/jobs/GUID GET
     Get the current state of a job. This may include aggregated breakdown of node state, (n nodes
     completed, m nodes failed, etc)
**** ORGNAME/pushy/jobs/GUID DELETE
     Delete a job, and the associated data
**** ORGNAME/pushy/jobs/GUID/nodes
     List all nodes in the job. 
***** TODO: does this also list the state, at least in short form? Probably should
**** ORGNAME/pushy/jobs/GUID/node/NODE
     Detailed information about a nodes status in the job. (Do we really need this level of detail?)

*** Knife job plugin
    Works against the above API.
