#+TITLE: Push Job Specification
#+AUTHOR: Christopher Brown <cb@opscode.com>\\Kevin Smith <kevin@opscode.com>
#+OPTIONS: toc:nil
#+EXPORT_DATE: nil
#+OPTIONS: ^:{}
#+LaTeX: \raggedright
#+LaTeX: \thispagestyle{fancy}
#+LaTeX_HEADER: \hypersetup{colorlinks=true,linkcolor=blue,linkbordercolor=blue}
#+LaTeX_HEADER: \def\@pdfborder{0 0 1}
#+LaTeX_HEADER: \def\@pdfborderstyle{/S/U/W 1}}
#+LaTeX_HEADER: \usepackage[T1]{fontenc}
#+LaTeX_HEADER: \usepackage[adobe-utopia]{mathdesign}
#+LaTeX_HEADER: \usepackage[scaled]{berasans}
#+LaTeX_HEADER: \usepackage[scaled]{beramono}
#+LATEX_HEADER: \setlength{\evensidemargin}{0in}
#+LATEX_HEADER: \setlength{\oddsidemargin}{0in}
#+LATEX_HEADER: \setlength{\textwidth}{6.5in}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage{paralist}
#+LATEX_HEADER: \pagestyle{fancy} \renewcommand{\headrulewidth}{0pt}
#+LATEX_HEADER: \chead{\includegraphics[width=4cm]{Opscode_Logo_Small.png}}
#+LATEX_HEADER: \lhead{} \rhead{} \lfoot{\today}
#+LATEX_HEADER: \cfoot{Opscode Confidential}\rfoot{\thepage}
#+LATEX_HEADER: \renewcommand{\headrulewidth}{0pt}
#+LaTeX_HEADER: \let\itemize\compactitem
#+LaTeX_HEADER: \let\description\compactdesc
#+LaTeX_HEADER: \let\enumerate\inparaenum

* Overview
  This specification describes the ability to execute on-demand chef-node via knife. For brevity's sake
  this feature will be referred to as "push job" or "push jobs".

  The concept of push jobs is quite simple. A user is able to select some subset of nodes managed by
  their Private Chef organization, specify an action or command for those nodes to execute, and
  track the status of each node as it executes the request.

  A bit of complexity and bookkeeping lurks underneath push job's simplicity. Managed nodes
  will need to a way to reliably monitor the job coordinators so they can cope with outages.
  Job coordinators also need to monitor the status of each managed node so commands are only
  sent to available nodes and to track job progress.

  The push job server and the managed nodes keep tabs on each other via a bidirectional heartbeating
  system. The push job server sends heartbeat messages to all the managed nodes, and the managed
  nodes send heartbeat messages to the server.

  The remainder of this document will attempt to describe this feature in enough detail to allow
  a reasonable and scalable implementation.

* Assumptions
** Connectivity
   1. Managed nodes *MUST* be reachable via a TCP-enabled network interface.
   2. Managed nodes *MUST* be able to accept incoming TCP connections.
   3. Managed nodes *MUST* be able to connect to the heartbeat and job coordination
      components inside Chef server.
** Data format & Storage
   1. All messages will be formatted as legal JSON.
   2. The database is the canonical store of all application data.
** Scalability & Security
   1. Push jobs will be deployed in Private Chef only.
   2. Push jobs will not be deployed in Hosted Chef.
   3. The design must scale up to 8,000 managed nodes per OPC server.
   4. Push jobs will honor the same security guarantees made by the Chef REST API.
* Architecture
** Communications
   Managed nodes and server components will communicate using [[http://www.zeromq.org][ZeroMQ]] messaging. While the server
   will be bound to predefined ports, the node should never require them. This is to ease running
   multiple nodes on the same host for scalability testing; we will want to be able to run many
   hundreds of nodes on the same machine rather than stand up thousands of individual nodes to
   test message scalability.

   Communication can be separated into two categories: heartbeat and job execution. The heartbeat
   channel is used by the Chef server to detect when managed nodes are offline and, therefore,
   unavailable for job execution. Managed nodes also use the heartbeat channel to detect when the
   server is unavailable.

   The job execution channel is used by the Chef server to send job execution requests to
   managed nodes. Managed nodes used the execution channel to send job-related messages such
   as acknowledging jobs, sending progress updates, and reporting final results.

*** Heartbeat Channel
    PUB and SUB sockets are used because they automatically and scalably manage the fanin
    the server requires to monitor nodes as well as the fanout the server needs to broadcast
    its heartbeat to all the nodes.

    In the interest of simplicity the heartbeat protocol should be as stateless as possible. The
    server continually broadcasts heartbeat messages to all nodes that are listening. Nodes
    indicate their availability by sending heartbeat messages to the server. There is no process
    beyond the zeromq connection to start or stop a connection from the node.

    This might be worth modifying in the future. For example we might want to lower the signature
    validation load on the server by having a per-session symmetric key for the node heartbeat
    established at startup. Instead of the somewhat expensive public key signature check we could
    simply decrypt the packet with the session key and check for sanity. (TODO: think about whether
    this protocol is actually sane and secure)

    #+CAPTION: ZeroMQ sockets
    #+LABEL: img:heartbeat.jpg
    #+ATTR_LaTeX: wrap placement={left}    [[./heartbeat.jpg]]

    The details of how nodes and servers discover and connect to each other's PUB and SUB sockets
    is covered in [[Configuration/Discovery Process]].
*** Command Channel
    TBD
** Configuration/Discovery Process
   The configuration and service discovery process will provide the following pieces of data:
   * The push job server hostname or address
   * The port to subscribe to for server heartbeat
   * The port to push node heartbeats to
   * The public key of the server
   * The lifetime of this configuration information

   A configuration endpoint ':organization/nodes/push_jobs' will be added to our chef rest
   services. A signed GET to this endpoint will retrieve the appropriate configuration information
   in JSON format.

   #+begin_src javascript
    {
      "type": "config",
      "host": "opc1.opscode.com",
      "push_jobs": {
                     "heartbeat": {
                                    "out_addr": "tcp://10.10.1.5:10000",
                                    "in_addr": "tcp://10.10.1.5:10001",
                                    "interval": 15,
                                    "offline_threshold": 3,
                                    "online_threshold": 2
                                  },
                   },
      "public_key": "AAAAB3NzaC1kc3MAAACBAIZbwlySffbB
                    5msSUH8JzLLXo/v03JBCWr13fVTjWYpc
                    cdbi/xL3IK/Jw8Rm3bGhnpwCAqBtsLvZ
                    OcqXrc2XuKBYjiKWzigBMC7wC9dUDGwDl
                    2aZ89B0jn2QPRWZuCAkxm6sKpefu++VPR
                    RZF+iyZqFwS0wVKtl97T0gwWlzAJYpAAA
                    AFQDIipDNo83e8RRp7Fits0DSy0DCpwAA
                    AIB01BwXg9WSfU0mwzz/0+5Gb/TMAxfkD
                    yucbcpJNncpRtr9Jb+9GjeZIbqkBQAqwg
                    dbEjviRbUAuSawNSCdtnMgWD2NXkBKEde",
       "lifetime":3600

    }
    #+end_src

    + type :: message type
    + host :: sender's host name (Private Chef server)
    + push\_jobs/heartbeat/out_addr :: URL pointing to the server's heartbeat broadcast service
    + push\_jobs/heartbeat/in_addr :: URL pointing to the server's node state tracking service
    + push\_jobs/interval :: Interval, in seconds, between heartbeat messages
    + push\_jobs/offline_threshold :: How many intervals must be missed before the other end is considered offline
    + push\_jobs/online_threshold :: How many intervals must be missed before the other end is considered online
    + public_key :: The signing key that the push server will use.
    + lifetime :: how long in seconds this configuration is good for. The node should reload the
                  configuration information after this has expired.

   We may wish to use the discovery process to handle failover to a new server and distribution of
   nodes among multiple servers. The discovery system would allocate the nodes to various active servers
   and if a node lost the server heartbeat for a certain length of time (or got a reconfigure
   command via the command channel) it would reload the configuration and start a connection to the
   appropriate server. We would also reconfigure after the liftime of the configuration expires.

** General Messaging
*** JSON
   Push jobs use JSON because ZeroMQ handles packet fragmentation and reassembly. JSON also
   facilitates easier debugging and maintenance of the system since all messages are textual
   and human-readable. A binary protocol, such as Protocol Buffers or msgpack, would be more
   efficient but would also substantially increase the effort required to debug and support
   the system.
*** Security
   All messages are signed using the caller's private key. This signature is transmitted in
   a separate ZeroMQ frame before the JSON payload.[fn:1]
   #+begin_src erlang
   Sock = connect_to_server("tcp://some_server:8765"),
   Sig = sign_message(JSON),
   erlzmq:send(Sock, Sig, [sndmore]),
   erlzmq:send(Sock, JSON)
   #+end_src
** Heartbeat
    Liveness detection in a distributed system is a notoriously difficult problem. The most common
    approach is to arrange for two parties to exchange heartbeat messages on a regular interval. Let's
    call these two parties 'A' and 'B'. Both A and B are considered 'online' while they are able to
    exchange heartbeat messages. If A fails to receive heartbeats from B for some number of consecutive
    intervals then A will consider B 'offline' and not route any traffic to B. A will update B's
    status to 'online' once A starts receiving heartbeats from B again.

    The protocol described here is loosely based on the Paranoid Pirate Protocol, but with some
    embelishments introduced because of the need for signing.  

    The heartbeat server sends out regular heartbeats to managed nodes via ZeroMQ PubSub. Managed
    nodes send their heartbeats over a separate channel. See the [[Heartbeat Channel]]
    section for a visual representation of the message flows and ZeroMQ sockets
*** Message Format
    The basic message format used here is a simple header packet containing the protocol version and
    a signature separated by CRLF.  

    The main packet is a JSON blob. Push jobs use JSON because ZeroMQ handles packet fragmentation
    and reassembly. JSON also facilitates easier debugging and maintenance of the system since all
    messages are textual and human-readable. A binary protocol, such as Protocol Buffers or msgpack,
    would be more efficient but would also substantially increase the effort required to debug and
    support the system.

*** Security
    All messages are signed using the caller's private key. This signature is transmitted in a
    separate ZeroMQ frame before the JSON payload. The actual payload is not encrypted, and is broad
    cast to all nodes. The system should never broadcast any data that is sensitive, such as
    commands or node status. This implies that the server heartbeat broadcast is not suitable for
    commands.

*** Socket configuration
    The heartbeats (and other messages) flowing through the system are time sensitive. There is
    little value keeping many more packets than the online/offline threshold values. If we go too
    long without receiving a heartbeat, we will be declaring the machine down anyways. Furthermore,
    the signing protocol will mandate the rejection of aged packets.

    This implies that the HWM values should be kept small, and ZMQ_SWAP should always be zero.

*** Server Heartbeat Channel
    PUB/SUB sockets are used for the server heartbeat because this manages the fanout required to
    send messages to thousands of nodes. The node subscribes to the server heartbeat at a
    host/port combination specified in the discovery process.

    The HWM should be kept small; there is no point in storing messages for dead nodes any longer
    than necessary. Nodes going down must be accepted and tolerated. If a node is not reachable
    for any length of time we want to drop those messages. This is in keeping with the fail fast
    philosphy.

    The nodes do not ACK the server heartbeats, and the server should not expect any.
*** Server Heartbeat Message
    Server heartbeat messages are comprised of two ZeroMQ frames. The first frame contains the
    signature version and checksum of the message body:

#+begin_example
    Version=1.0;Checksum=fyq6ukIwYcUJ9JI90Ets8Q==
#+end_example

    The second frame contains the JSON-formated heartbeat payload:

#+begin_example
    {"server": "opc.foo.com",                     # specifies the server's host name
     "type": "heartbeat",                         # message type
     "timestamp":"Sat, 12 May 2012 20:33:15 GMT"  # timestamp in RFC1123 format
    }
#+end_example
*** Node Heartbeat Channel
   PUSH/PULL sockets are used for the node hearbeat. The node PUSHes heartbeats to the server at
   the host/port specified in the config data received during [[Server and Node Discovery][discovery]]. The server will not ACK
   heartbeats.

   NOTE: Some versions of this spec had PUB/SUB being used for this process. It would be simpler if
   the node was able to connect to the server to send heartbeats, rather than requiring the server
   to bind to the node. The latter would require some sort of handshake on startup to inform the
   server where to connect. While it is possible to bind the SUB to an address and connect the PUB,
   this seems to be not recommended (see zeromq guide, 'Getting the Message Out'). However, it seems
   that multiple PUSH to one PULL is supported, and we can bind the PULL socket to an address
   without trouble.

   There isn't any reason we couldn't use the heartbeat to convey extra information. The public key
   signature based authentication process for heartbeats already requires a moderate sized payload,
   so a little extra information seems pretty harmless. This is in contrast to the 1-2 byte sized
   payload in the paranoid pirate protocol. Possible items to include are:

   * The port the command processor is listening on.
   * ID and status of the most recently received command.
   * Information allowing the detection of crashed nodes
*** Node Heartbeat Message
    Node heartbeats are comprised of two ZeroMQ frames. The first frame contains
    the signature version and signature for the heartbeat message:

#+begin_example
    VersionId=1.0;Checksum=CUtCvLKeS0l/fq5lnGf25w==
#+end_example

    The second frame contains the JSON-formatted heartbeat payload:

#+begin_example
    {"node": "node123.foo.com",                    # node's host name
     "org": "foo.com",                             # orgname of the node
     "timestamp": "Sat, 12 May 2012 20:33:15 GMT", # timestamp
     "command_port": 9987                          # the port we are listening on for commands
    }
#+end_example

   The node will discontinue the heartbeat and note the server as down if the server heartbeat
   state moves to down, and resume it when the server heartbeat resumes. 

   A managed node must mark the OPC server as offline when it fails to receive server heartbeats for
   a consecutive number of intervals equal to push\_jobs/heartbeat/offline\_threshold. A managed
   node must not attempt to send any data when the server is offline. Any job requests received by
   the managed node from the offline server which haven't begun execution must be discarded.
 
   After a managed node has marked the server as offline it must receive server heartbeats for a consecutive
   number of intervals equal to push\_jobs/heartbeat/online\_threshold before marking the server online.
   The managed node may resume sending data and accepting job requests from the OPC server at this point.

   If the node fails to recieve a heartbeat for too long, it will query the configuration
   interface to receive a possible configuration update. This would allow the system to recover from
   a failed server.

   The node may wish to detect if the HWM is reached on the PUSH socket, since it will block when the
   HWM is reached. One strategy would be to set the HWM low and have some sort of alarm detect if we
   are blocked for any length of time. If the HWM is reached, we should declare the server down as
   if it stopped sending heartbeats. 
** Node-Server Command Execution
    A OPC server sends requests to execute actions to managed nodes. These requests are
    called commands.
*** TODO How does a node advertise its PUSH socket?
*** TODO What happens if a node goes offline mid-job? What if it's an expected outage (reboot)?
*** TODO What is the default duration for a job?
*** TODO Does OPC have a notion of max #/concurrently executing push jobs?
*** TODO How does the job monitor monitor the up/down state of each participating node?
*** TODO How do we handle job queueing for backed up nodes? No more than 1-2 per node?
*** Vocabulary
    * Job - A collection of nodes executing the same command. Jobs have persistent state.
      This state can be queried by users via the knife 'job' command.
    * Command - A request to a managed node to perform an action immediately.
*** Commands and node state
     A server may not send commands to a down node. A server may not send a command to
     a node which is currently executing a command. In other words, nodes execute
     serially only. This makes it easier to reason about the current state of any node
     and also avoids any undesired runtime interactions between commands.
*** Command vocabulary
***** chef-client
      This command causes a managed node to initiate a chef-client run.
***** ohai
      This command tells a managed node to run ohai and update its node data.
      This is not required for the PoC project but will be required for the real
      implementation.
*** Server-side job execution lifecycle
**** Initiation
***** Search & Filter
     Command execution is tracked across a collection of nodes by a command job. The
     collection of nodes is defined by a standard Chef search criteria. The search
     is executed against the node index and returns a list of managed nodes satisfying
     the search criteria. Any nodes which are marked down at the time of job initiation
     shall be omitted from the job.
***** Command Transmission
     By default the job shall be executed on all candidates returned by the search
     and filter step. Exceptions to this and user-visible ways to modify the
     default behavior are discussed later in this document.

     When the OPC server is ready to execute a job it creates a job monitor. The role
     of the job monitor is to manage the execution of a job and track the execution
     state of all nodes referenced by the job. The job monitor has two associated
     ZeroMQ sockets: one each of PUSH and PULL.

     The PUSH socket is used to transmit commands to the nodes participating in the job.
     The PULL socket is used to receive job ACK/NAK traffic from the nodes.
****** Command message format
       The job command message is extremely similar to the heartbeat message. It
       consists of two ZeroMQ frames. The first frame contains the signature version and
       signature of the message.

#+begin_example
Version=1.0;Checksum=vQiGY85BzEAi5k7noSiN3A==
#+end_example

       The second frame contains the command payload for the job formatted as a JSON
       hash. The payload contains the job id, command to be executed, the host name
       of the OPC server, and the URL of the PULL socket used to collect node ACK/NAKs.

#+begin_example
{
  "server": "opc1.foo.com",               # OPC host name
  "type": "job_command",                  # message type
  "job_id": "1234",                       # job id
  "listener": "tcp://opc1.foo.com:15037", # PULL socket endpoint
  "command": "chef-client"                # command to execute
}
#+end_example

***** Modifying Job Initiation
      Users can place additional restrictions on the initiation of a push job.
      These restrictions are expressed by passing additional flags to the knife
      job command.
****** Quorum
       A user can specify that a minimum number of nodes matching the search criteria
       must both be online and ACK the job request. This is done by using the quorum
       flag.

#+begin_example
knife job create role:database chef-client --quorum=0.8
#+end_example

#+begin_example
knife job create role:database chef-client --quorum=3
#+end_example

       The first example illustrates specifying quorum as a percentage of the total
       number of nodes returned from the search index.

       The second example illustrates specifying quorum as an absolute count of nodes.
**** Executing the job
     Once a job meets the job initiation criteria it is ready to execute. The job monitor
     tells the participating nodes to begin execution by sending the execute message to
     all ACK'd nodes.
***** Execute message format
      As with prior messages, the execute message is comprised of two ZeroMQ frames.
      The first frame contains the digital signature and signature version as previously
      illustrated. The second frame contains the execute message payload.

#+begin_example
{
  "server": "opc1.foo.com",    # OPC host name
  "job_id": "1234",            # job id
  "type": "job_execute"        # message type
}
#+end_example
**** Tracking job status
      If at least one node has ACK'd the job request (see exceptions to this rule in
      subsequent sections) the monitor then transitions the job to an executing state.
      The monitor is responsible for updating the job details as each participating node
      sends its job execution results. Since OPC already has other mechanisms for collecting
      the output of chef-client runs, nodes are required to report only pass/fail status.
***** Job States
      The following is a list of valid job states:

      * Executing - At least one node ACK'd the job request and is executing the command.
      * Complete - All participating nodes executed the command without errors.
      * Error - At least one node encountered errors while executing the command.
      * Failed - All candidate nodes NAK'd the job request OR no candidate nodes were found.
      * Expired - At least one node failed to complete in the specified time.
****** Job Expiry
       Users can also specify a maximum duration for a job. This is accomplished by passing
       the duration flag to the knife job plugin. Duration is always expressed in minutes.

#+begin_example
knife job create role:database chef-client --duration=10
#+end_example

*** Node-side job execution lifecycle
**** Initiation
     ACK if idle. NAK if job is currently running and the server asked node to start
     another one. If ACK, consider the job to have started execution for the purposed
     of future job requests.
**** Execution
     Wait for server to send execution message. Begin command execution when message is
     received.
**** Completion
     Send success or error message when command completes.

*** Knife job plugin
