# -*- fill-column: 100 -*-
#+TITLE: Push Job Specification for V1 Product
#+AUTHOR: Mark Anderson <mark@opscode.com>\\Christopher Brown <cb@opscode.com>\\Kevin Smith <kevin@opscode.com>
#+OPTIONS: toc:2
#+EXPORT_DATE: nil
#+OPTIONS: ^:{}
#+LaTeX: \raggedright
#+LaTeX: \thispagestyle{fancy}
#+LaTeX_HEADER: \def\@pdfborder{0 0 1}
#+LaTeX_HEADER: \def\@pdfborderstyle{/S/U/W 1}
#+LaTeX_HEADER: \usepackage[T1]{fontenc}
#+LaTeX_HEADER: \usepackage[adobe-utopia]{mathdesign}
#+LaTeX_HEADER: \usepackage[scaled]{berasans}
#+LaTeX_HEADER: \usepackage[scaled]{beramono}
#+LATEX_HEADER: \setlength{\evensidemargin}{0in}
#+LATEX_HEADER: \setlength{\oddsidemargin}{0in}
#+LATEX_HEADER: \setlength{\textwidth}{6.5in}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage{paralist}
#+LATEX_HEADER: \pagestyle{fancy} \renewcommand{\headrulewidth}{0pt}
#+LATEX_HEADER: \chead{\includegraphics[width=2cm]{Opscode_Logo_Small.png}}
#+LATEX_HEADER: \lhead{} \rhead{} \lfoot{\today}
#+LATEX_HEADER: \cfoot{Opscode Confidential}\rfoot{\thepage}
#+LATEX_HEADER: \renewcommand{\headrulewidth}{0pt}

# Workflow for emitting PDF
# Export as latex (c-c c-e l)
# Generate PDF (/usr/local/texlive/2011/bin/x86_64-darwin/pdflatex design.tex on my system)

* Overview
Opscode Chef is currently missing a key feature that comes up in discussion with nearly
every potential customer.  That feature is immediate, ad-hoc task execution against one or
more managed devices (nodes).  In Chef, we will model these tasks as /run lists/ and will
model their execution by distributed, coordinated /chef-client/ runs. For brevity's sake
this feature will be referred to as /push jobs/.

The concept of /push jobs/ is quite simple. A user is able to select some subset of nodes
managed by their Private Chef organization, specify an action or command for those nodes
to execute, and track the status of each node as it executes the request.

** A simple use case

Imagine that we are administering a large collection of machines running various web stacks.
We need to patch some, but not all of our systems. You *could* try:
#+BEGIN_EXAMPLE
% knife ssh "role:my_role" chef-client
#+end_example
Being able to use search to specify the set of nodes is nice, but it gets unwieldy to track
things if you have more than a few systems. It gets especially painful if some of the commands
fail, or some systems are unreachable.

We want to define a set of nodes, issue commands to them, and take actions on them based on
the results of prior commands.

#+begin_example
% knife push job start "role:my_role" chef-client
Starting job id 235
#+end_example

Then the status could be tracked, in detail:
#+begin_example
% knife push job status 235
Node name  Status    Last updated
foo        Failed    2012-05-04 00:00
bar        Done      2012-05-04 00:01
#+end_example

Or in the aggregate:
#+begin_example
% knife push job status 235 --summary
 1   node acknowledged, not started
 1   node not available, busy
 1   node not available, down
14   nodes started
 4   nodes completed with errors
20   nodes completed successfully
#+end_example

Or individually:
#+begin_example
% knife push job status 235 node foo
node foo Failed running job id 235 at 2012-05-04 00:00
#+end_example

New jobs can be started from the results of prior jobs.
#+begin_example
% knife push job start --job_id=235 --job_status=failed chef-client
Starting job id 236
#+end_example

** Internals
A bit of complexity and bookkeeping lurks underneath push job's simplicity. Managed nodes
will need to reliably monitor the job coordinators to cope with outages.  Job coordinators
also need to monitor the status of each managed node so commands are only sent to available,
responding nodes and to track job progress.

The push job server and the managed nodes ensure mutual liveness via a bidirectional
heartbeating system. The push job server sends heartbeat messages to all the managed nodes,
and the managed nodes send heartbeat messages to the server.

The remainder of this document will attempt to describe the elements for the V1 release in
enough detail to allow a reasonable and scalable implementation.

* Assumptions
** Connectivity
   1. Managed nodes *MUST* be reachable via a TCP-enabled network
      interface.
   2. Managed nodes *MUST* be able to authenticate and connect to the
      Chef REST API
   3. Managed nodes *MUST* be able to connect to the heartbeat and job coordination
      components inside Chef server.
** Data format & Storage
   1. All messages will be formatted as legal JSON.
   2. The database is the canonical store of all application data.
** Scalability & Security
   1. Push jobs will be deployed in Private Chef only; no Hosted Chef
   2. Push jobs will not be deployed in Hosted Chef.
   3. The design must scale up to 8,000 managed nodes per OPC server.
   4. Push jobs will honor the same security guarantees and
      limitations made by the Chef REST API.
      

* Architecture
** Communications
   Managed nodes and server components will communicate using [[http://www.zeromq.org][ZeroMQ]] messaging. The server will bind
   its endpoints to well known ports, whose addresses will be part of the client configuration
   process. The clients will connect to those ports, initiating all communications. The sole
   exception will be initial configuration, which uses the Chef REST API to discover the addresses
   and ports for the push job server.

   This structure simplifies running clients behind NAT routers. It eases scalability
   testing; we will want to be able to run many hundreds of nodes on the same machine rather than
   stand up thousands of individual nodes. 

   All messages are signed with the sender's private key using a protocol similar to the Chef REST
   API protocol. (This might be worth modifying in the future. For example we might lower the
   signature validation load on the server by having a per-session symmetric key for the node
   heartbeat established at startup, and use a symmetric signing protocal such as HMAC.)

   There are two basic kinds of messages in the system: heartbeat and command. 

   Communication can be separated into two categories: heartbeat and job execution. Heartbeat
   messages are used by the Chef server to detect when managed nodes are offline and, therefore,
   unavailable for job execution. Managed nodes use the server heartbeat messages to detect when the
   server is unavailable, and discontinue their heartbeats until the server returns.

   Command messages are used by the Chef server to send job execution requests to managed
   nodes. Managed nodes use command messages to send job-related messages such as acknowledging
   jobs, sending progress updates, and reporting final results.

*** Heartbeating
    The ZeroMQ PUB/SUB (publish and subscribe) pattern is used for the server heartbeat because it
    automatically and scalably manage the fan-out the server needs to broadcast its heartbeat to all
    the nodes. The server will bind a PUB socket to a port and advertise this via the configuration
    API.

    Clients heartbeat to the server using a DEALER/ROUTER pattern. The server will bind a ROUTER
    socket to a port and advertise this via the configuration API. Current client heartbeats use the
    same socket as the command channel; this not only saves a connection (a worthwhile goal when we
    have the possibilty of 10,000 clients), but it serves to 'pilot' the connection. Since the
    client connects to the server, without the heartbeat the client would have to periodically send
    a message through on the command channel to set up the connection.

#+CAPTION: ZeroMQ sockets
#+LABEL: img:heartbeat
[[[[./heartbeat.jpg]]]]

*** Command Channel

    The ZeroMQ ROUTER/DEALER pattern will be used for the command channel. The server will bind a
    ROUTER socket to a port, and advertise this via the configuration API. The clients will connect
    to this via DEALER sockets. 

    As discussed above, we could use a separate channel for client heartbeat messages and command
    messages. This would require clients will send signed messages to the server announcing their
    availability for commands. This is necessary for the server to create a binding between the
    zeromq routing address and a particular pushy client. 

** Configuration/Discovery Process

   There a substantial amount of data specific to pushy that needs to be distributed to the push job
   clients. We don't want to have to configure this on every node. Furthermore, much of this data is
   specific to the chef infrastructure, not the users infrastructre, and may change. 
   
   We assume every pushy client will start with a valid chef client key and the address of the chef
   REST service. The chef server will provide a REST endpoint to provide the configuration
   information needed to bootstrap the node to a usable state. A signed /GET/ to this endpoint will
   retrieve the appropriate configuration information in JSON format.

   The configuration and service discovery process will provide the following pieces of data:
   + The port to subscribe to for server heartbeat
   + The port to push node heartbeats to and use for commands
   + The public key of the server
   + The lifetime of this configuration information

   We may wish to use the discovery process to handle fail-over to a new server and distribution of
   nodes among multiple servers. The discovery system would allocate the nodes to various active servers
   and if a node lost the server heartbeat for a certain length of time (or got a reconfigure
   command via the command channel) it would reload the configuration and start a connection to the
   appropriate server. We would also reconfigure after the lifetime of the configuration expires.


*** Socket configuration
    The ZeroMQ messages flowing through the system are time sensitive. For example, if we go
    too long without receiving a heartbeat, we will be declaring the machine down
    anyways. There is little value keeping many more packets than the online/offline threshold
    values.  Furthermore, the signing protocol will mandate the rejection of aged packets.

    The HWM should be kept small (1 would be a good value); there is no point in storing
    messages for dead nodes any longer than necessary. ZMQ_SWAP should always be zero. Node
    failure must be accepted and tolerated. If a node has been marked as down (not reachable),
    we want to drop any messages destined for that node. This is in keeping with the fail-fast
    philosophy.

#+INCLUDE: "design_heartbeat.org" :minlevel 0

* Command Execution
   A OPC server sends requests to execute actions to managed nodes. These requests are called
   commands. The command server listens on an address specified in the configuration process, and
   clients connect to that address to receive commands.

   TODO: Discuss idempotence of commands; desireable, but not required.

   Only one command at a time can be executing on a node. In other words, nodes execute
   commands serially. This makes it easier to reason about the current state of any node and
   also avoids any undesired runtime interactions between commands.

** Vocabulary
    * Job - A collection of nodes executing the same command. Jobs have persistent state.
      This state can be queried by users via the knife 'job' command.
    * Command - A request to a managed node to perform an action immediately.
    * Jobset - the set of nodes in the job (Is there a better name this thing)

** Overall communications structure
   The command server will create a ROUTER socket bound to a port, and each client will
   connect via a DEALER socket. On connection to the ROUTER socket, the client will send a
   signed message indicating that it is available for commands. The message  letting us capture the
   transient socket name. This will provide a way to map the unique name of the connection to
   the client in question. Commands will be addressed via that unique name. The client will
   learn the the address of the ROUTER socket via the discovery system.

   The server will need to send a separate message to each client. However the message body
   (aside from the address packet) will remain the same, and we can reuse the ZeroMQ buffers created
   and save on the signing cost.

   When a command is completed on the node, it sends the command results back to the server using
   the ROUTER/DEALER connection above.

*** Open Questions
    + What happens to commands sent to permanently dead nodes; does it case a leak sitting in
      the buffering. Is dropping and restarting ZMQ a solution?

#+INCLUDE: "design_client_tracking.org" :minlevel 2

** Job lifecycle

    A job is started by passing a job description, a set of nodes (jobset), and an initiating
    user id to the pushy server. Each node will flow through a series of states as it executes
    the job ending in either *OK*, or *FAILED*. These states are final; once a node enters it
    stays there.

    The job description includes a command, and optional values including:
    + The quorum required to start the job.
    + A timeout for the overall job execution.

    Possible extensions include:
    + A timeout for execution on a single node.
    + A limit on how many nodes can be executing simultaneously. We may want to update a
      bunch of nodes, but limit how many of them are out of service simultaneously
    + A limit on the rate at which nodes may be started. This is to limit load on the
      server. Some of this can be handled by API throttling, but this allows more fine grained
      control and can be used to run a job 'in the background'.
    + A timeout for how long we wait to achieve quorum
    + A timeout for how long to wait for an ACK before marking a node dead (or perhaps for any
      message from the server)

    Throughout the process nodes may drop heartbeat, be marked down and return. The final node
    state depends on where it was in the execution process.

*** Specification of the nodeset
    Initially the set will be specified explicitly as a list of nodes to the server.

    Search, both against the chef index and against state from previous jobs will be
    implemented via the knife command. The collection of nodes is defined by a standard Chef
    search criteria. The search is executed against the node index and returns a list of
    managed nodes satisfying the search criteria. The search criteria should eventually allow
    up/down state and job execution state as factors. For example it would be quite valuable
    to run a job against all nodes that match a search and 'succeeded' on job id XXXX.

    As an optimization we will eventually want to avoid the round trip transmission of large
    node lists to the knife command and back to the server by having the server accept search
    terms directly but that is left for a future version.

    Entity groups (named groups of nodes) will greatly ease the specification of node lists,
    and we may want to implement them as part of the first pushy implementation.

*** Permission checking
    We will check that the user id has permissions to execute a job, and whether they have job
    execution permissions on each node in the set. The simplest model is to add a group 'push
    job initiators' to the system, and require the user to be in that group.

    In addition we will probably want fine grained administrative control, either at the
    environment level or per node level. We probably should add an new permission *EXECUTE* to
    the current CRUDG set to allow fine grained control.

    Any node for which permisions are denied with be marked as *FAILED* with 'permission
    denied' as the reason.

*** Availability checking
    If a node is down at the start of the job, it will be marked as *FAILED* with 'down' as
    the reason.

    The server will send the remaining nodes a *PREPARE* message. Nodes that are available for
    execution will respond with *ACK* and be marked as *READY*, while those unavailable for
    execution respond with *NACK*. Nodes that *NACK* will be marked as *FAILED* with 'busy'
    and the job id in the state.  Even though nodes may finish with their current work and
    become available while the job is still in progress, we are treating *NACK* as a permanent
    condition for simplicity's sake

    If a node drops heartbeat or otherwise goes down prior to starting it is marked as
    *FAILED* with reason 'down'.

    There are two ways to handle quorum and command execution.

    The simplest is to wait a predetermined time for the nodes to *ACK/NACK*. If a quorum is
    achieved we start, otherwise we fail. If a node fails to *ACK* in time, we mark it timed
    out, and issue a cancel command to the node.

    A more sophisticated version is to track nodes transitioning to *READY*, and when the
    quorum is reached we start execution. If the quorum is not reached, we never start the
    job. We still need some sort of timeout for ACK/NACK, as some nodes may not transition to
    ready, holding the job open until it times out or is aborted.

    If two users are running jobs against the same set of nodes, some nodes may be affiliated
    with one job, and some with another. This may cause us to not reach quorum, or run a
    particular command on one subset and a different command on the rest. The assumption is
    that it is preferrable to make some forward progress rather than inflict a global lock on
    jobs. If this is an issue it should be avoided by setting high quorum requirements.

*** Execution
    Once quorum is achieved we start sending *START* messages. The naive implementation would
    just issue messages as quickly as possible to those nodes that have *ACK*ed at the start
    of execution.

    A more refined version would be throttled by various limits on simultaneous execution and
    peak initation, so we may not start all of the nodes at once. There will also be late
    arriving nodes who *ACK* after execution begins. This implies that the execution tracker
    will need to be able to handle streaming nodes through the running state. This would also
    be helpful for pipelined job execution.

    Nodes reply to the *START* message with a *STARTED* acknowledgement


*** Completion
    As nodes complete, we mark them as completed and *OK* state, or *FAILED* with an error
    state derived from the command.

    The job is marked as complete when all nodes report in as *OK* or *FAILED*, or the overall
    job times out.

*** Cleanup and error handling.

    Throughout the execution of the job, nodes may fail in various ways. The server will mark
    them *FAILED*. Nodes cannot be relied on to be updated as failed, or know that their role
    in the job is over, so a process is needed to insure tha node cleanup happens
    properly. For example if a node *NACK*s because it thinks it is still running a job that
    is over, we should be able to reset it and make it available for new work.

    The the pushy client does not retain job state over reboots. The client reports its
    current state via heartbeat messages, including the current job ID and state. The server
    should check the heartbeat message, and send an abort command if the node is running a
    dead job.

    The overall philosophy for error handling should be fail fast, and don't try to do exotic
    recovery code. Instead of handling every possible case and trying to recover the workflow
    should support easily issuing jobs based on the failure status, If the commands are
    idempotent, they can be rerun harmlessly.

    If a node crashes or restarts, that terminates execution of the job. We do not attempt to
    resume jobs afterwards. We should distinguish controlled restarts from outright crashes,
    and signal the server when we detect a shutdown in progress.

    If a node disappears (loses heartbeat), it should be marked *FAILED*. If the node returns,
    the server should send an explicit *ABORT* message. If it has not started the reason
    should be 'down'. If it has started the reason will 'crashed while executing'. While
    sometimes nodes might reconnect, and even have successful completed status it is simpler
    to treat them as failed.

*** Modifing running jobs
    There are reasons to allow the currently running job to be altered in flight.

**** Aborting
     Jobs must be able to be aborted after they've started. This is accomplished by sending an
     abort message to each node in the job that has not already completed. If a node already
     started, it is marked 'aborted'. No guarantees can be made about the progress or state of
     nodes which have already started. If the node hasn't already started, it is marked 'not
     started'. In future versions we may want to have a distinction between hard and soft
     aborts, where a soft abort allows commands that have started to run to completion. It may
     also be worth allowing aborts to apply to a subset of the nodes in the job.

**** Adding nodes to a running job
     In future versions we may want to add nodes to a running job. If searches are made more
     dynamic, or entity groups are dynamic sets, we will want to update the job with new
     nodes, and have them go through the same life cycle. This would require the streaming
     mode of command execution.

**** Changing parameters
     In future versions we will want to allow modification the timeouts and quora requirements
     of a started job. For example, we may have a job running slower than expected; it's
     making forward progress but is at risk of timing out. The user very likely would like to
     alter the job time out to prevent the job from ending in a failed with timeout state.

*** Persistence and lifetime

    Jobs need to be persistent on the server side; we should not 'forget' a job in progress even if
    the server restarts. However, clients may lose state at any time, and we must be prepared to
    handle it. Jobs (and the collection of node statuses in the job) persist until explicitly
    deleted.

** Command Vocabulary

   The first version of the protocol will use a restricted command vocabulary with the option
   of arbitrary commands. Which commands to allow is a policy decision, and probably should be
   configurable on a per organization basis.

   + chef-client
    This command causes a managed node to initiate a chef-client run. When the one shot
    runlist feature is added to chef-client we will want to allow that as an option
   + ohai
    This command tells a managed node to run ohai and update its node data.
   + arbitrary command?
    This is the most general possible solution, and something we should consider for the long
    term; apparently (according to Lamont) just about every company has some provision for this in
    their infrastructure. Many interesting security issues appear, including what UID to run under
    (root? the user id that knife ssh would use?) access control problems, etc.

    Perhaps this needs to have a specific ACL right in authz separate from the rest of the
    commands. At minimum, this should be configurable on a per-org basis, preferably via the
    discovery REST endpoint.
   + sleep n
    This command tells a managed node to wait n seconds and then reply with success. This is
    intended for testing.
   + dummy_job PFAIL DURATION
    This would be a more sophisticated version of the sleep command. This command tells a
    managed node to wait DURATION seconds and either fail with a probability of PFAIL or
    succeed. This would ease testing of the system failure cases.
   + abort JOBID
    (TODO Does this belong here? Is this really a command?)
    The final product will need some mechanism to cancel/abort a job in progess. The job id may
    not be necessary since there will only be one job running per client. (However it might be
    nice if the client and server get out of sync, and the client thinks its running a different
    job than the server does)

*** special command types
    There are a few types of commands that have interesting properties, and should be
    considered in future versions.

    + Informational jobs. While most commands are run for their side effects, we may want to
      run some sort of information only command and return its results for future jobs. For
      example we could run a job that checks for a kernel version and returns the result. A
      new job could be started to update the nodes for which the result was a particular
      version. The simplest kind would simply return a boolean predicate, but more
      sophisticated queries could be added. However this risks turning into a generalized
      search language.
    + Restarting
      Some commands may cause restarts on their own (e.g. running some MSI installers on
      Windows). The client may want to detect a proper system shutdown and report that to the
      job runner. Resumption of the command after shutdown is tricky, and we should probably
      limit expectations to commands whose last actions are restarts. Managing this on diverse
      platforms will require a bit of work.

      Otherwise we may also want an explicit restart command in our vocabulary. This would
      allow chaining of jobs where there is work to do prior to the restart, a restart, and work that
      must be accomplished after the restart.


** Client command state machine
|-------------+------------------------------------+--------------------------------+------------------|
| State       | Event                              | Action                         | Next State       |
|-------------+------------------------------------+--------------------------------+------------------|
| Startup     | Detect clean shutdown              |                                | Pending          |
|             | Previously running message         | Send job failed msg            | Pending          |
|             | Hard shutdown detected             | Send crash msg                 | Pending          |
|             | Defer message processing until     |                                |                  |
|-------------+------------------------------------+--------------------------------+------------------|
| Pending     | Server heartbeat found             |                                | Ready            |
|-------------+------------------------------------+--------------------------------+------------------|
| Ready       | entry to state                     | On entry send ready message    |                  |
|             | Prepare message                    | Send ACK                       | Waiting (Job Id) |
|             | Status message                     | Send Status                    | Ready            |
|             | Abort message                      | Ignore                         | Ready            |
|             | Start message                      | Send Start failed              | Ready            |
|             | Server heartbeat loss              |                                | Pending          |
|-------------+------------------------------------+--------------------------------+------------------|
| Waiting     | Prepare message (job id matches)   | Send ACK                       | Waiting          |
|             | Prepare message (job id doesn't)   | Send NACK                      | Waiting          |
|             | Status message                     | Send Waiting info              | Waiting          |
|             | Abort message                      | Send aborted (from waiting)    | Ready            |
|             | Start message (job id matches)     | Send Started, exec message     | Running          |
|             | Start message (job id doesn't)     | Send Error                     | Waiting          |
|             | Timeout on ready period            | Send Error                     | Ready            |
|             | Server restart detected            | Send Error                     | Ready            |
|-------------+------------------------------------+--------------------------------+------------------|
| Running     | Prepare message                    | Send NACK                      | Running          |
|             | Status message                     | Send Running info              | Running          |
|             | Abort message                      | Send aborted                   | Terminating      |
|             | Start message (job id matches)     | Ignore                         | Running          |
|             | Start message (job id doesn't)     | Send error (can't start)       | Running          |
|             | Current exec'd command succeeds    | Send completed, success        | Ready            |
|             | Current exec'd command fails       | Send completed, message failed | Ready            |
|             | Current command exceeds time bound | Send completed, timed out      | Terminating      |
|             | Server restart detected            | Send Running info              | Running          |
|-------------+------------------------------------+--------------------------------+------------------|
| Terminating | Prepare message                    | Send NACK                      | Terminating      |
|             | Status message                     | Send Terminating info          | Terminating      |
|             | Abort message                      | Send error (can't abort)       | Terminating      |
|             | Start message                      | Send error (can't start)       | Terminating      |
|             | Current exec'd command succeeds    | Send completed, success        | Ready            |
|             | Current exec'd command fails       | Send completed, failed         | Ready            |
|-------------+------------------------------------+--------------------------------+------------------|

   TODO:
   + Server heartbeat loss
   + Server heartbeat return
   + Server restart detected
   + Stuck process detected (more complicated on windows)
   + Add timeout for waiting state.

** Server Job Control Logic

The server is responsible for tracking job and node state and moving jobs forward.
The following is a full description of the state the server tracks and the events
that cause jobs to move forward.

All actions taken by the server are governed by state transitions (triggered by
network messages or timeouts).  This allows us to isolate and reason about logic
in a nice, discrete way.  We save state on every transition: this allows us to
to recover after a crash.  We load a job in a given state and set things up so
that we can exit the state the same way as before.

There are three types of object we track state for: jobs, nodes, and job-nodes.

*** Events

All state transitions are triggered by events.  This is a list of all source
events in the system.  Note that most state machine changes are proximately
triggered by changes in other machines' state.

- REST API:: the user can affect jobs through the REST API.
  - Start Job: this creates a job, and initializes it in voting state.
  - Abort Job: this triggers an abort for a job.
- Node Reports:: nodes report in over ZeroMQ periodically, telling us about
  things they are doing / have done.
  - Heartbeat: nodes periodically tell us their state with this.
  - ACK/NACK: response to a command message, saying whether the node
    can or cannot run a command.  ack means node state is now "ready."
  - Started: response to a start message.  Means node state is now "executing."
  - Completed: when executing is finished.  Means node state is now "idle."
  - Aborted: response to an abort message.  Means node state is now "idle."
- Node Down Detection:: we detect whether a node is down or flapping based on
  how often it sends heartbeat messages.  "node up" happens when a heartbeat
  occurs.
- Timeouts:: there are timeouts for each phase of a job:
  - Voting timeout: when voting times out, quorum fails and we abort the job.
  - Executing timeout: when execution times out, we abort the job.
  - Aborting timeout: when this finishes, we mark the job aborted regardless of
    whether nodes have actually responded that they have aborted.  This timeout
    must be at least as big as the stampede skew and the "node down detection"
    time.  TODO: do we need this timeout?  Can we rely entirely on node down
    detection to do the job for us?

*** Node Execution State

This represents the server's knowledge of the node's current progress with
respect to its current job.  A node can only have one job associated with it,
and this state machine tracks that.

The primary states are IDLE -> READY -> EXECUTING and movement into these
states is *always* driven by reports from the node (they are always reflective
of things the node told us).

The ABORTING state is an aspirational state where we're trying to get the node
to abort.

|-----------+------------------------------------+--------------------------|
| State     | Event                              | Action                   |
|-----------+------------------------------------+--------------------------|
| IDLE      | Node Report -> ready               | READY                    |
| IDLE      | Node Report -> other               | ->ABORTING               |
|-----------+------------------------------------+--------------------------|
| READY     | Node Report -> executing+this job  | ->EXECUTING              |
| READY     | Node Report -> other+non-ready     | ->ABORTING               |
| READY     | Job State -> ABORTING/FINISHED     | ->ABORTING               |
|-----------+------------------------------------+--------------------------|
| EXECUTING | Node Report -> idle+executed this  | finish->IDLE             |
| EXECUTING | Node Report -> ready+other job     | finish->READY            |
| EXECUTING | Node Report -> executing this      | <nothing>                |
| EXECUTING | Node Report -> other+non-executing | ->ABORTING               |
| EXECUTING | Job State -> ABORTING/FINISHED     | ->ABORTING               |
|-----------+------------------------------------+--------------------------|
| ABORTING  | Enter/Load State                   | Send abort message       |
| ABORTING  | Node Report -> idle+last job=this  | finish->IDLE             |
| ABORTING  | Node Report -> ready               | finish->READY            |
| ABORTING  | Node Report -> anything else       | resend                   |
| ABORTING  | Node State -> up                   | resend                   |
|-----------+------------------------------------+--------------------------|

"Enter Job" in any other state than VOTING will cause the Node Execution state machine to

When the Node State moves to up, VOTING, TRIGGERED and ABORTING states will resend their message.

*** Job State

This represents the aggregate state of the job as it moves through its three
major phases (voting, executing and finished).  It is affected by timeouts and
REST commands, and aggregates calculated from job state and node up/down.

Jobs keep track of which nodes have agreed to be part of it (READY state), which
nodes have finished the job, and whether the nodes are up or down.  These events
affect Aggregate Detection and caused it to run.

|-------------+----------------------------------+-----------------------|
| State       | Event                            | Action                |
|-------------+----------------------------------+-----------------------|
| VOTING      | Initializing or loading state    | Send command to nodes |
| VOTING      | Quorum Success                   | EXECUTING             |
| VOTING      | Quorum Failure                   | ABORTING              |
| VOTING      | Voting Timeout                   | ABORTING              |
| VOTING      | non-voted Node State -> up       | Resend command to node|
|-------------+----------------------------------+-----------------------|
| EXECUTING   | Initializing or loading state    | Send start to nodes   |
| EXECUTING   | All Nodes Exited                 | FINISHED              |
| EXECUTING   | Execution Timeout                | ABORTING              |
| EXECUTING   | acked Node State -> up           | Resend start to node  |
|-------------+----------------------------------+-----------------------|
| FINISHED    |                                  |                       |
|-------------+----------------------------------+-----------------------|
| ABORTING    | All Nodes Exited                 | FINISHED              |
| ABORTING    | Aborting Timeout                 | FINISHED              |
|-------------+----------------------------------+-----------------------|

TODO what if the client missed the command or start message because it restarted?

**** Aggregate Detection

     The Job state machine keeps a tally of which nodes acknowledged the job and
     which nodes are down, so that it can calculate some tallies and take care
     of quorum conditions:
     
     - Quorum Success:: if we are voting, and "ready" nodes > X%.
     - Quorum Failure:: if we are voting, and nacked/down nodes > Y% or there are no nodes.
     - All Nodes Exited:: if all nodes that started the job are finished or down.

     On initialization, Aggregate Detection subscribes to node status for all nodes in the
     job, and Aggregate Detection listens for NACK, Node State=Up/Down, and Node Execution
     State = READY/FINISHED, and updates the list of nodes accordingly.

*** Node State

    A node can be in up or down state.  This machine listens for heartbeats and
    uses timers and statistics to decide whether a machine is presently UP or DOWN.

    Node status is always sent to the Node Execution State, and jobs can subscribe
    to node status.
    TODO: describe new algorithm.

#+INCLUDE: "design_rest_api.org" :minlevel 0

#+INCLUDE: "design_zeromq_messages.org" :minlevel 0

* Knife command syntax

We will need syntax to allow users to create, alter and delete jobs, find out the state of jobs in flight in both
summary and detailed fashion, and find out the jobs associated with a node.

** Job create

*** Modifying Job Initiation
      Users can place additional restrictions on the initiation of a push job.
      These restrictions are expressed by passing additional flags to the knife
      job command.
*** Quorum
       A user can specify that a minimum number of nodes matching the search criteria
       must both be online and ACK the job request. This is done by using the quorum
       flag.
#+begin_example
knife job create role:database chef-client --quorum=0.8
#+end_example

#+begin_example
knife job create role:database chef-client --quorum=3
#+end_example

       The first example illustrates specifying quorum as a percentage of the total
       number of nodes returned from the search index.

       The second example illustrates specifying quorum as an absolute count of nodes.

*** Lifetime
       A job will have a specific lifetime; if execution has not completed by the timeout, nodes with a command in
       flight will be aborted and the job state will be marked as timed out. There should also be a default lifetime of
       a job set to some TBD value. (Is an hour a reasonable time? Most chef-client runs should be done by then). There
       are obvious tradeoffs between squelching laggarts and not being too hasty.

*** Concurrency
       In many cases we will want to limit how many simultaneous nodes are executing a job. This will complicate the job
       manager, as it will need to track nodes completing or timing out and start new nodes.

****** Job Expiry
       Users can also specify a maximum duration for a command on a
       single node. This is accomplished by passing
       the duration flag to the knife job plugin. Duration is always expressed in minutes.

#+begin_example
knife job create role:database chef-client --duration=10
#+end_example

* TODO Add future work section here [2012-06-20 Wed]
